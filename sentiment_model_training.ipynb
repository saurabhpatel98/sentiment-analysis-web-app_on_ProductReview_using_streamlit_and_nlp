{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auggvjlPbRxN"
   },
   "source": [
    "## Plan of Action\n",
    "\n",
    "\n",
    "1.   We are using **Amazon Alexa Reviews dataset (3150 reviews)**, that contains: **customer reviews, rating out of 5**, date of review, Alexa variant \n",
    "2.   First we  **generate sentiment labels: positive/negative**, by marking *positive for reviews with rating >3 and negative for remaining*\n",
    "3. Then, we **clean dataset through Ventorization Feature Engineering** (TF-IDF) - a popular technique\n",
    "4. Post that, we use **Support Vector Classifier for Model Fitting** and check for model performance (*we are getting >90% accuracy*)\n",
    "5. Last, we use our model to do **predictions on real Amazon reviews** using: a simple way and then a fancy way\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z5wIxkEUaKx"
   },
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "BQgQHj_g_Hrc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (67.4.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (8.0.16)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy) (1.22.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.2/12.8 MB 3.9 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.5/12.8 MB 4.9 MB/s eta 0:00:03\n",
      "     -- ------------------------------------- 0.8/12.8 MB 5.4 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 1.1/12.8 MB 5.6 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 5.8 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.9/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.1/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.4/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.7/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.0/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 3.9/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.1/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.4/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.7/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.0/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.8/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.1/12.8 MB 6.1 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.4/12.8 MB 6.1 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.6/12.8 MB 6.1 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.9/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.8/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.1/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.4/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.0/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.8/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.1/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.9/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.2/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.5/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 6.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.16)\n",
      "Requirement already satisfied: setuptools in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (67.4.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.22.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saura\\downloads\\compressed\\sentiment_analysis_with_sklearn_pipeline-main\\venv\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl#egg=en_core_web_sm==3.3.0 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 736,
     "status": "ok",
     "timestamp": 1643786326411,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "f-H7y2SIwWh3",
    "outputId": "a6997a18-86f2-4b0d-ddef-f0a4b1c81a6a"
   },
   "source": [
    "Set your working directory  here\n",
    "\n",
    "If you are using Google Colab, use this code snippet:\n",
    "\n",
    "```from google.colab import drive\n",
    "drive.mount('/content/drive')```\n",
    "\n",
    "```%cd /content/drive/My Drive/Project6_SentimentAnalysis_with_Pipeline```\n",
    "\n",
    "If you are working locally on PC, keep training data in the same directory as this code file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1643786412453,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "zwnN_CQGzRJM",
    "outputId": "bed5aba9-3cd4-4bd6-d6ff-f5578f3ac949"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Company</th>\n",
       "      <th>Product</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Clean_Title</th>\n",
       "      <th>Clean_Review</th>\n",
       "      <th>cleaned_title_review</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Apple iPhone XS, US Version, 64GB, Space Gray</td>\n",
       "      <td>Honestly, it was worth it</td>\n",
       "      <td>22-Jun-19</td>\n",
       "      <td>5</td>\n",
       "      <td>I was very hesitant about buying an iPhone off...</td>\n",
       "      <td>honestly worth</td>\n",
       "      <td>hesitant buying iphone amazon disappoint come ...</td>\n",
       "      <td>honestly worth hesitant buying iphone amazon d...</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.6901</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Apple iPhone XS, US Version, 64GB, Space Gray</td>\n",
       "      <td>Eh wouldn’t buy again</td>\n",
       "      <td>30-Jun-19</td>\n",
       "      <td>1</td>\n",
       "      <td>One - It comes in a weird boxTwo it had more s...</td>\n",
       "      <td>eh wouldnt buy</td>\n",
       "      <td>one come weird boxtwo scuff scratch id like pr...</td>\n",
       "      <td>eh wouldnt buy one come weird boxtwo scuff scr...</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.548</td>\n",
       "      <td>-0.3536</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Apple iPhone XS, US Version, 64GB, Space Gray</td>\n",
       "      <td>Beautiful, lovely, practically brand new iPhon...</td>\n",
       "      <td>04-Feb-20</td>\n",
       "      <td>5</td>\n",
       "      <td>I absolutely love my new iPhone XS! It arrived...</td>\n",
       "      <td>beautiful lovely practically brand new iphone x</td>\n",
       "      <td>absolutely love new iphone x arrive time pract...</td>\n",
       "      <td>beautiful lovely practically brand new iphone ...</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Apple iPhone XS, US Version, 64GB, Space Gray</td>\n",
       "      <td>Phone not working</td>\n",
       "      <td>25-Dec-18</td>\n",
       "      <td>4</td>\n",
       "      <td>The phone is froze up and unable to use. Very ...</td>\n",
       "      <td>phone work</td>\n",
       "      <td>phone froze unable use poor productedit within...</td>\n",
       "      <td>phone work phone froze unable use poor product...</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Apple iPhone XS, US Version, 64GB, Space Gray</td>\n",
       "      <td>May be defective one</td>\n",
       "      <td>02-Jul-19</td>\n",
       "      <td>1</td>\n",
       "      <td>Suddenly Wifi is not working properly and i co...</td>\n",
       "      <td>may defective one</td>\n",
       "      <td>suddenly wifi work properly could see phone fe...</td>\n",
       "      <td>may defective one suddenly wifi work properly ...</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.648</td>\n",
       "      <td>-0.3415</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45907</th>\n",
       "      <td>45907</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>Samsung Galaxy Note 10+, 256GB, Aura WhiteFully</td>\n",
       "      <td>The phone was as described on amazon.</td>\n",
       "      <td>16-May-20</td>\n",
       "      <td>5</td>\n",
       "      <td>It was a Birthday gift for my wife and she lov...</td>\n",
       "      <td>phone describe amazon</td>\n",
       "      <td>birthday gift wife love</td>\n",
       "      <td>phone describe amazon birthday gift wife love</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45908</th>\n",
       "      <td>45908</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>Samsung Galaxy Note 10+, 256GB, Aura WhiteFully</td>\n",
       "      <td>The phone is great</td>\n",
       "      <td>14-Feb-20</td>\n",
       "      <td>5</td>\n",
       "      <td>The phone itself is great. There are no issues...</td>\n",
       "      <td>phone great</td>\n",
       "      <td>phone great issue whatsoever mine come perfect...</td>\n",
       "      <td>phone great phone great issue whatsoever mine ...</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.9123</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45909</th>\n",
       "      <td>45909</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>Samsung Galaxy Note 10+, 256GB, Aura WhiteFully</td>\n",
       "      <td>Beautiful.</td>\n",
       "      <td>14-May-21</td>\n",
       "      <td>5</td>\n",
       "      <td>Like new condition. No scratches. Boots up as ...</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>like new condition scratch boot att say fully ...</td>\n",
       "      <td>beautiful like new condition scratch boot att ...</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45910</th>\n",
       "      <td>45910</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>Samsung Galaxy Note 10+, 256GB, Aura WhiteFully</td>\n",
       "      <td>Juvenile Star Wars model</td>\n",
       "      <td>17-Jun-21</td>\n",
       "      <td>3</td>\n",
       "      <td>This particular model, the Galaxy Note 10+, is...</td>\n",
       "      <td>juvenile star war model</td>\n",
       "      <td>particular model galaxy note 10 heavy compare ...</td>\n",
       "      <td>juvenile star war model particular model galax...</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.753</td>\n",
       "      <td>-0.7882</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45911</th>\n",
       "      <td>45911</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>Samsung Galaxy Note 10+, 256GB, Aura WhiteFully</td>\n",
       "      <td>DONT HESITATE TO PURCHASE!!!</td>\n",
       "      <td>26-Nov-20</td>\n",
       "      <td>5</td>\n",
       "      <td>Wasn't expecting much for the price but I'm bl...</td>\n",
       "      <td>dont hesitate purchase</td>\n",
       "      <td>wasnt expect much price im blow away quality c...</td>\n",
       "      <td>dont hesitate purchase wasnt expect much price...</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.7850</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45912 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  Company                                           Product  \\\n",
       "0          0    Apple    Apple iPhone XS, US Version, 64GB, Space Gray    \n",
       "1          1    Apple    Apple iPhone XS, US Version, 64GB, Space Gray    \n",
       "2          2    Apple    Apple iPhone XS, US Version, 64GB, Space Gray    \n",
       "3          3    Apple    Apple iPhone XS, US Version, 64GB, Space Gray    \n",
       "4          4    Apple    Apple iPhone XS, US Version, 64GB, Space Gray    \n",
       "...      ...      ...                                               ...   \n",
       "45907  45907  Samsung  Samsung Galaxy Note 10+, 256GB, Aura WhiteFully    \n",
       "45908  45908  Samsung  Samsung Galaxy Note 10+, 256GB, Aura WhiteFully    \n",
       "45909  45909  Samsung  Samsung Galaxy Note 10+, 256GB, Aura WhiteFully    \n",
       "45910  45910  Samsung  Samsung Galaxy Note 10+, 256GB, Aura WhiteFully    \n",
       "45911  45911  Samsung  Samsung Galaxy Note 10+, 256GB, Aura WhiteFully    \n",
       "\n",
       "                                                   Title       Date  Rating  \\\n",
       "0                              Honestly, it was worth it  22-Jun-19       5   \n",
       "1                                  Eh wouldn’t buy again  30-Jun-19       1   \n",
       "2      Beautiful, lovely, practically brand new iPhon...  04-Feb-20       5   \n",
       "3                                      Phone not working  25-Dec-18       4   \n",
       "4                                   May be defective one  02-Jul-19       1   \n",
       "...                                                  ...        ...     ...   \n",
       "45907              The phone was as described on amazon.  16-May-20       5   \n",
       "45908                                 The phone is great  14-Feb-20       5   \n",
       "45909                                         Beautiful.  14-May-21       5   \n",
       "45910                           Juvenile Star Wars model  17-Jun-21       3   \n",
       "45911                       DONT HESITATE TO PURCHASE!!!  26-Nov-20       5   \n",
       "\n",
       "                                                  Review  \\\n",
       "0      I was very hesitant about buying an iPhone off...   \n",
       "1      One - It comes in a weird boxTwo it had more s...   \n",
       "2      I absolutely love my new iPhone XS! It arrived...   \n",
       "3      The phone is froze up and unable to use. Very ...   \n",
       "4      Suddenly Wifi is not working properly and i co...   \n",
       "...                                                  ...   \n",
       "45907  It was a Birthday gift for my wife and she lov...   \n",
       "45908  The phone itself is great. There are no issues...   \n",
       "45909  Like new condition. No scratches. Boots up as ...   \n",
       "45910  This particular model, the Galaxy Note 10+, is...   \n",
       "45911  Wasn't expecting much for the price but I'm bl...   \n",
       "\n",
       "                                           Clean_Title  \\\n",
       "0                                       honestly worth   \n",
       "1                                       eh wouldnt buy   \n",
       "2      beautiful lovely practically brand new iphone x   \n",
       "3                                           phone work   \n",
       "4                                    may defective one   \n",
       "...                                                ...   \n",
       "45907                            phone describe amazon   \n",
       "45908                                      phone great   \n",
       "45909                                        beautiful   \n",
       "45910                          juvenile star war model   \n",
       "45911                           dont hesitate purchase   \n",
       "\n",
       "                                            Clean_Review  \\\n",
       "0      hesitant buying iphone amazon disappoint come ...   \n",
       "1      one come weird boxtwo scuff scratch id like pr...   \n",
       "2      absolutely love new iphone x arrive time pract...   \n",
       "3      phone froze unable use poor productedit within...   \n",
       "4      suddenly wifi work properly could see phone fe...   \n",
       "...                                                  ...   \n",
       "45907                            birthday gift wife love   \n",
       "45908  phone great issue whatsoever mine come perfect...   \n",
       "45909  like new condition scratch boot att say fully ...   \n",
       "45910  particular model galaxy note 10 heavy compare ...   \n",
       "45911  wasnt expect much price im blow away quality c...   \n",
       "\n",
       "                                    cleaned_title_review  Positive  Negative  \\\n",
       "0      honestly worth hesitant buying iphone amazon d...     0.246     0.197   \n",
       "1      eh wouldnt buy one come weird boxtwo scuff scr...     0.173     0.279   \n",
       "2      beautiful lovely practically brand new iphone ...     0.325     0.033   \n",
       "3      phone work phone froze unable use poor product...     0.266     0.133   \n",
       "4      may defective one suddenly wifi work properly ...     0.120     0.232   \n",
       "...                                                  ...       ...       ...   \n",
       "45907      phone describe amazon birthday gift wife love     0.688     0.000   \n",
       "45908  phone great phone great issue whatsoever mine ...     0.339     0.095   \n",
       "45909  beautiful like new condition scratch boot att ...     0.348     0.000   \n",
       "45910  juvenile star war model particular model galax...     0.099     0.148   \n",
       "45911  dont hesitate purchase wasnt expect much price...     0.251     0.087   \n",
       "\n",
       "       Neutral  Compound Sentiment  \n",
       "0        0.556    0.6901  positive  \n",
       "1        0.548   -0.3536  negative  \n",
       "2        0.641    0.9896  positive  \n",
       "3        0.601    0.2732  positive  \n",
       "4        0.648   -0.3415  negative  \n",
       "...        ...       ...       ...  \n",
       "45907    0.312    0.8316  positive  \n",
       "45908    0.567    0.9123  positive  \n",
       "45909    0.652    0.7506  positive  \n",
       "45910    0.753   -0.7882  negative  \n",
       "45911    0.662    0.7850  positive  \n",
       "\n",
       "[45912 rows x 15 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the dataset\n",
    "dump = pd.read_csv('sentiment.csv',sep=',')\n",
    "\n",
    "dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eS_QZCeEX45f"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 713,
     "status": "ok",
     "timestamp": 1643786466114,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "VlIYFkfUzbMx",
    "outputId": "ac39a217-8c5c-43cd-b3a5-a2addb2d9be7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was very hesitant about buying an iPhone off...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One - It comes in a weird boxTwo it had more s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I absolutely love my new iPhone XS! It arrived...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The phone is froze up and unable to use. Very ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suddenly Wifi is not working properly and i co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0  I was very hesitant about buying an iPhone off...          5\n",
       "1  One - It comes in a weird boxTwo it had more s...          1\n",
       "2  I absolutely love my new iPhone XS! It arrived...          5\n",
       "3  The phone is froze up and unable to use. Very ...          4\n",
       "4  Suddenly Wifi is not working properly and i co...          1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dump[['Review','Rating']]\n",
    "dataset.columns = ['Review', 'Sentiment']\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "NOZZDqWe0zKZ"
   },
   "outputs": [],
   "source": [
    "# Creating a new column sentiment based on overall ratings\n",
    "def compute_sentiments(labels):\n",
    "  sentiments = []\n",
    "  for label in labels:\n",
    "    if label > 3.0:\n",
    "      sentiment = 1\n",
    "    elif label <= 3.0:\n",
    "      sentiment = 0\n",
    "    sentiments.append(sentiment)\n",
    "  return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1643786499319,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "pIjR4QPo04Aw",
    "outputId": "a9ce863d-65c6-4244-b255-398c1d1bf91a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saura\\AppData\\Local\\Temp\\ipykernel_16396\\4076257393.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['Sentiment'] = compute_sentiments(dataset.Sentiment)\n"
     ]
    }
   ],
   "source": [
    "dataset['Sentiment'] = compute_sentiments(dataset.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 711,
     "status": "ok",
     "timestamp": 1643786525514,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "KoyOZ3Ha1AGz",
    "outputId": "437e0031-e939-4f82-ef45-82ce42a7d324"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was very hesitant about buying an iPhone off...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One - It comes in a weird boxTwo it had more s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I absolutely love my new iPhone XS! It arrived...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The phone is froze up and unable to use. Very ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suddenly Wifi is not working properly and i co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0  I was very hesitant about buying an iPhone off...          1\n",
       "1  One - It comes in a weird boxTwo it had more s...          0\n",
       "2  I absolutely love my new iPhone XS! It arrived...          1\n",
       "3  The phone is froze up and unable to use. Very ...          1\n",
       "4  Suddenly Wifi is not working properly and i co...          0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 715,
     "status": "ok",
     "timestamp": 1643786544244,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "tmBZbWOCR-p3",
    "outputId": "0b85503c-e17a-4b4f-ab71-a4f38a10ace5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    31540\n",
       "0    14372\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check distribution of sentiments\n",
    "\n",
    "dataset['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1643786549233,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "_Cuulg5nT9Fd",
    "outputId": "332978ab-5d9f-4613-df72-b436791c53fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review       0\n",
       "Sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "dataset.isnull().sum()\n",
    "\n",
    "# no null values in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uOoNB4vUhiv"
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "XCPpUC--BNLW"
   },
   "outputs": [],
   "source": [
    "x = dataset['Review']\n",
    "y = dataset['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "pfe9U2w2AUrI"
   },
   "outputs": [],
   "source": [
    "# Create a function to clean data \n",
    "# We shall remove stopwords, punctuations & apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "xiut-KzPUjyx"
   },
   "outputs": [],
   "source": [
    "# import string\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ZTfcWsAuWCSN"
   },
   "outputs": [],
   "source": [
    "from custom_tokenizer_function import CustomTokenizer\n",
    "\n",
    "# # creating a function for data cleaning\n",
    "\n",
    "# def text_data_cleaning(sentence):\n",
    "#   doc = nlp(sentence)                         # spaCy tokenize text & call doc components, in order\n",
    "\n",
    "#   tokens = [] # list of tokens\n",
    "#   for token in doc:\n",
    "#     if token.lemma_ != \"-PRON-\":\n",
    "#       temp = token.lemma_.lower().strip()\n",
    "#     else:\n",
    "#       temp = token.lower_\n",
    "#     tokens.append(temp)\n",
    " \n",
    "#   cleaned_tokens = []\n",
    "#   for token in tokens:\n",
    "#     if token not in stopwords and token not in punct:\n",
    "#       cleaned_tokens.append(token)\n",
    "#   return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "uKsnnULWzDAn"
   },
   "outputs": [],
   "source": [
    "# if root form of that word is not pronoun then it is going to convert that into lower form\n",
    "# and if that word is a proper noun, then we are directly taking lower form,\n",
    "# because there is no lemma for proper noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1643787692930,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "VHgVcoYFyStv",
    "outputId": "b6e175ea-228c-4465-dda2-8fecd3e25cf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'beautiful', 'day', 'outside']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do a test\n",
    "custom_tokenizer = CustomTokenizer()\n",
    "custom_tokenizer.text_data_cleaning(\"Hello all, It's a beautiful day outside there!\")\n",
    "# stopwords and punctuations removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gb-4E3Xj0MZI"
   },
   "source": [
    "### Vectorization Feature Engineering (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "EHlWW9D10XwB"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "3OZuzO9c0mhq"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=custom_tokenizer.text_data_cleaning)\n",
    "# tokenizer=text_data_cleaning, tokenization will be done according to this function\n",
    "#cleaning operations such as removing stopwords, punctuation, and other non-alphabetic characters, and then return a list of tokens (words) that are ready for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaJPf_MWAiUk"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkEiNIFwBfgM"
   },
   "source": [
    "### Train/ Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Qeqrb1Es1LyF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, stratify = dataset.Sentiment, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1643787711353,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "DUX2gNXl1Q8c",
    "outputId": "75e6f894-c489-425d-a942-6646cc845d9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36729,), (9183,))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape\n",
    "# 2520 samples in training dataset and 630 in test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrCsAstOAwa6"
   },
   "source": [
    "### Fit x_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "kgu3SPK61G3T"
   },
   "outputs": [],
   "source": [
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "iuVdiRzP26sc"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('tfidf',tfidf), ('clf',classifier)])\n",
    "# it will first do vectorization and then it will do classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31315,
     "status": "ok",
     "timestamp": 1643787756540,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "IpxdCk475DKW",
    "outputId": "9759e67b-f457-4bfe-db63-47ae143d833c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(tokenizer=&lt;bound method CustomTokenizer.text_data_cleaning of &lt;custom_tokenizer_function.CustomTokenizer object at 0x0000024BA0476EC0&gt;&gt;)),\n",
       "                (&#x27;clf&#x27;, LinearSVC())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(tokenizer=&lt;bound method CustomTokenizer.text_data_cleaning of &lt;custom_tokenizer_function.CustomTokenizer object at 0x0000024BA0476EC0&gt;&gt;)),\n",
       "                (&#x27;clf&#x27;, LinearSVC())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(tokenizer=&lt;bound method CustomTokenizer.text_data_cleaning of &lt;custom_tokenizer_function.CustomTokenizer object at 0x0000024BA0476EC0&gt;&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<bound method CustomTokenizer.text_data_cleaning of <custom_tokenizer_function.CustomTokenizer object at 0x0000024BA0476EC0>>)),\n",
       "                ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "oAH9-v355Jua"
   },
   "outputs": [],
   "source": [
    "# in this we don't need to prepare the dataset for testing(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 704,
     "status": "ok",
     "timestamp": 1643787897201,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "ZHJQPw77YrxV",
    "outputId": "b7ad94b4-e386-469e-c3e1-7ca6e00ebb11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentiment_model.pkl']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(pipeline,'sentiment_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCVFR5A25qaS"
   },
   "source": [
    "## Check Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "xp3Nj55h5smN"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "i2yugURA50Td"
   },
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 945,
     "status": "ok",
     "timestamp": 1643788126203,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "xvt59nSM58ee",
    "outputId": "f0ae541a-ea74-4cc0-cf9e-56447abc8f8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2398,  477],\n",
       "       [ 380, 5928]], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 761,
     "status": "ok",
     "timestamp": 1643788133762,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "CoX7Jabd6Ar1",
    "outputId": "b9ecc0ee-334d-4016-ffbb-8aa3a9536dc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85      2875\n",
      "           1       0.93      0.94      0.93      6308\n",
      "\n",
      "    accuracy                           0.91      9183\n",
      "   macro avg       0.89      0.89      0.89      9183\n",
      "weighted avg       0.91      0.91      0.91      9183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "# we are getting almost 91% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1643788151989,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "d5cosLk16ChG",
    "outputId": "fe3f4b10-9975-4862-eaed-d45636e57e9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.67"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(accuracy_score(y_test, y_pred)*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpo6jTiNaXYu"
   },
   "source": [
    "## Predict Sentiments using Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDfwV9SHa6ZI"
   },
   "source": [
    "### Simple way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 692,
     "status": "ok",
     "timestamp": 1643788219404,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "gYDMKDvcWzSk",
    "outputId": "577f75e1-2013-42cb-8ba7-a15d7a9d8329"
   },
   "outputs": [],
   "source": [
    "# prediction = pipeline.predict([\"Alexa is bad\"])\n",
    "\n",
    "# if prediction == 1:\n",
    "#   print(\"Result: This review is positive\")\n",
    "# else:\n",
    "#   print(\"Result: This review is negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QS1vi_onbJMh"
   },
   "source": [
    "### Fancy way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 125457,
     "status": "ok",
     "timestamp": 1643788362443,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "PWQUC8QmG6-T",
    "outputId": "b2db3a4c-0533-40c7-a239-81788b70dd41"
   },
   "outputs": [],
   "source": [
    "# new_review = []\n",
    "# pred_sentiment = []\n",
    "\n",
    "# while True:\n",
    "  \n",
    "#   # ask for a new amazon alexa review\n",
    "#   review = input(\"Please type an Alexa review (Type 'skip' to exit) - \")\n",
    "\n",
    "#   if review == 'skip':\n",
    "#     print(\"See you soon!\")\n",
    "#     break\n",
    "#   else:\n",
    "#     prediction = pipeline.predict([review])\n",
    "\n",
    "#     if prediction == 1:\n",
    "#       result = 'Positive'\n",
    "#       print(\"Result: This review is positive\\n\")\n",
    "#     else:\n",
    "#       result = 'Negative'\n",
    "#       print(\"Result: This review is negative\\n\")\n",
    "  \n",
    "#   new_review.append(review)\n",
    "#   pred_sentiment.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1643789450464,
     "user": {
      "displayName": "SKILLCATE",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhT5e7MnDmh2dcfNhKDOV8oRoeRJVinedzmD2Su=s64",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "VIend1TIMy1s",
    "outputId": "b7808856-f61f-4591-9d7e-d86ac2cc5843"
   },
   "outputs": [],
   "source": [
    "# Results_Summary = pd.DataFrame(\n",
    "#     {'New Review': new_review,\n",
    "#      'Sentiment': pred_sentiment,\n",
    "#     })\n",
    "\n",
    "# Results_Summary.to_csv(\"./predicted_sentiments.tsv\", sep='\\t', encoding='UTF-8', index=False)\n",
    "# Results_Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = joblib.load('sentiment_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the sentiment model to the reviews in the dataframe\n",
    "sentiments = []\n",
    "for review in df['Review']:\n",
    "    sentiment = model.predict([review])[0]\n",
    "    if sentiment == 1:\n",
    "        sentiments.append('positive')\n",
    "    else:\n",
    "        sentiments.append('negative')\n",
    "\n",
    "# Add the predicted sentiments to the dataframe\n",
    "df['result'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the updated dataframe as a new CSV file\n",
    "df.to_csv('sentiment_with_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "b1_SentimentAnalysis_with_Pipeline.ipynb",
   "provenance": [
    {
     "file_id": "1Dsyd5zB23upbCRDxlrh_r1P1wIlILCEa",
     "timestamp": 1642754413575
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
